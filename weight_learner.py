# -*- coding: utf-8 -*-
"""weight_learner.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c01htaVNgHR3osuXSHLSYxxpXQYn1Tyj
"""

import tensorflow as tf
import numpy as np
import json
import os
import codecs
import sqlite3
import io
from random import *


from tensorflow.examples.tutorials.mnist import input_data
fashion = input_data.read_data_sets('MNIST_data/fashion', one_hot=True)
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)


'''
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))
'''

def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)

def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)

def conv2d(x, W):
    return tf.nn.conv2d(
        x, W, strides=[1, 1, 1, 1],
        padding='SAME'
    )


def max_pool_2x2(x):
    return tf.nn.max_pool(
        x, ksize=[1, 2, 2, 1],
        strides=[1, 2, 2, 1], padding='SAME'
    )

def adapt_array(arr):
    out = io.BytesIO()
    np.save(out, arr)
    out.seek(0)
    return sqlite3.Binary(out.read())

def convert_array(text):
    out = io.BytesIO(text)
    out.seek(0)
    return np.load(out)

def train():
    with tf.Session() as sess:
        
        '''Train init network on MNIST Data'''
        con = sqlite3.connect(":memory:", detect_types=sqlite3.PARSE_DECLTYPES)
        cur = con.cursor()
        table_names = ['W_conv1', 'b_conv1', 'W_conv2', 'b_conv2', 'W_fc1', 
                    'b_fc1', 'W_fc2', 'b_fc2']
        
        for l in range(1):
            a = Weighter()            
            sess.run(tf.global_variables_initializer())
            print(a.W_conv1.eval())
            tt_names = [a.Wt_conv1, a.bt_conv1, a.Wt_conv2, a.bt_conv2, a.Wt_fc1, 
                    a.bt_fc1, a.Wt_fc2, a.bt_fc2]
            for i in range(500):
                try:
                    batch = mnist.train.next_batch(50)
                    a.train_step2.run(feed_dict={a.x: batch[0], a.y_: batch[1], a.keep_prob: 0.5})
                    if i % 50 == 0:
                        print 'step %d' % i
                    print('test accuracy %g' % a.error2.eval(feed_dict={
                        a.x: batch[0], a.y_: batch[1], a.keep_prob: 1.0}))
                except Exception as e:
                    print(e)
                    continue

            i = 0
            '''Train a network, store the weights into tables and retrieves
            them and puts them into the weights of test network - then runs the test network
            '''
            for var in a.tvars:
                values = var.eval()
                cur.execute("create table if not exists " +  table_names[i] + "(id integer PRIMARY KEY, arr array)")
                cur.execute("insert into " + table_names[i] + "(arr) values (?)", (values, ))
                i += 1
            del a
        a = Weighter()
        sess.run(tf.global_variables_initializer())
        tt_names = [a.Wt_conv1, a.bt_conv1, a.Wt_conv2, a.bt_conv2, a.Wt_fc1, 
                    a.bt_fc1, a.Wt_fc2, a.bt_fc2]
        i = 0
        for var in a.tvars:
            cur.execute("select arr from " + table_names[i] + " ORDER BY RANDOM() LIMIT 1")
            data = cur.fetchone()[0]
            print(table_names[i])
            try:
                sess.run(tt_names[i].assign(data))
            except Exception as e:
                print(e)
                continue
            i += 1
        print('test accuracy %g' % a.error1.eval(feed_dict={
                    a.x: batch[0], a.y_: batch[1], a.keep_prob: 1.0}))
        a.train_step1.run(feed_dict={a.x: batch[0], a.y_: batch[1], a.keep_prob: 0.5})
        
        
        '''set up and train the weight-learning networks + plug the finished weight into init network
           Then finally evaluate init network
        '''
        
        "'Training with combining all weights'"
        
        for step in range(0,1,1):
            print(step)
            tablelen = cur.execute("SELECT count(*) from W_conv1")
            blah = cur.fetchone()[0]
            rownum = randint(1,blah)
            shape_list = []
            len_list = []
            for h in range(0,len(table_names),1):
                #cur.execute("select arr from " + table_names[h] + " ORDER BY RANDOM() LIMIT 1")
                cur.execute("select arr from " + table_names[h] + " WHERE id = " + str(rownum))
                data = cur.fetchone()[0]
                shape = data.shape
                shape_list = shape_list + [shape]
                flat_dat = data.flatten()
                flat_len = len(flat_dat)
                len_list = len_list + [flat_len]
                if h == 0:
                    allweights = flat_dat
                else:
                    prevlen = len(allweights)
                    allweights = np.resize(allweights,(prevlen+len(flat_dat)))
                    flatstep = 0
                    for i in range(prevlen,len(allweights),1):
                        allweights[i] = flat_dat[flatstep]
                        flatstep += 1
            if step == 0:
                a.set_weights(len(allweights))
            sess.run(tf.global_variables_initializer())
            print('weights set')
            comp_data = np.reshape(allweights,(1,len(allweights)))
            batch = mnist.train.next_batch(50)
            a.train_stepslearn.run(feed_dict={a.x: batch[0], a.ylearn: comp_data, a.keep_prob: 0.5})
            if step % 10 == 0:
                print 'step %d' % step
                print('test accuracy %g' % a.errorlearn.eval(feed_dict={
                    a.x: batch[0], a.ylearn: comp_data, a.keep_prob: 1.0}))
        print('NOW PUTTING IN LEARNED WEIGHTS INTO NETWORK')
        final_weight = a.convlearn2.eval(feed_dict={
                        a.x: batch[0], a.keep_prob: 1.0})
        final_weight = final_weight.flatten()
        #final_weight = final_weight[0:flat_len]
        start = 0
        for tabs in range(0,len(table_names),1):
            end = start + len_list[tabs]
            temp_weight2 = allweights[start:end]
            temp_weight = final_weight[start:end]
            start = start + len_list[tabs]
            temp_weights = np.reshape(temp_weight, shape_list[tabs])
            sess.run(a.tvars[tabs].assign(temp_weights))
            print('final_eval %d') % tabs
            print(a.error2.eval(feed_dict={a.x: batch[0], a.y_: batch[1], a.keep_prob: 1.0})) 
        
        print('QED')
        print(a.W_conv1.eval())
        for i in range(2000):
                batch = mnist.train.next_batch(50)
                a.train_step2.run(feed_dict={a.x: batch[0], a.y_: batch[1], a.keep_prob: 0.5})
                if i % 50 == 0:
                    print('step %d' % i)
                    print('test accuracy %g' % a.error2.eval(feed_dict={
                        a.x: batch[0], a.y_: batch[1], a.keep_prob: 1.0})) 

        '''Training with weights seperated'''
        
        '''
        for count in range(0,len(table_names),1):
            sess.run(tf.global_variables_initializer())
            cur.execute("select arr from " + table_names[count] + " ORDER BY RANDOM() LIMIT 1")
            print(table_names[count])
            data = cur.fetchone()[0]
            shape = data.shape
            flat_dat = data.flatten()
            flat_len = len(flat_dat)
            a.set_weights(flat_len)
            sess.run(tf.global_variables_initializer())
            for k in range(100):
                try:
                    cur.execute("select arr from " + table_names[count] + " ORDER BY RANDOM() LIMIT 1")
                    data = cur.fetchone()[0]
                    comp_data = data.flatten()
                    comp_data = np.reshape(comp_data,(1,len(comp_data)))
                    batch = mnist.train.next_batch(50)
                    a.train_stepslearn.run(feed_dict={a.x: batch[0], a.ylearn: comp_data, a.keep_prob: 0.5})
                    if k % 10 == 0:
                        print 'step %d' % k
                        print('test accuracy %g' % a.errorlearn.eval(feed_dict={
                            a.x: batch[0], a.ylearn: comp_data, a.keep_prob: 1.0}))
                except Exception as e:
                    print(e)
                    continue
            
            final_weight = a.convlearn2.eval(feed_dict={
                            a.x: batch[0], a.keep_prob: 1.0})
            final_weight = final_weight.flatten()
            final_weight = final_weight[0:flat_len]
            print(shape)
            print(len(final_weight))
            final_weight = np.reshape(final_weight, shape)
            sess.run(a.tvars[count].assign(final_weight))

            print('Done')
            print(a.error2.eval(feed_dict={a.x: batch[0], a.y_: batch[1], a.keep_prob: 1.0}))
        ''' 
        

class Weighter():
    def __init__(self):
        '''data + weight labels'''
        self.x = tf.placeholder(tf.float32, shape=[None, 28*28], name='x')
        self.y_ = tf.placeholder(tf.float32, shape=[None, 10])

        '''Init Network Weights'''

        self.x_image = tf.reshape(self.x, [-1, 28, 28, 1])

        self.W_conv1 = weight_variable([5, 5, 1, 32])
        self.b_conv1 = bias_variable([32])

        self.h_conv1 = tf.nn.sigmoid(conv2d(self.x_image, self.W_conv1) + self.b_conv1)
        self.h_pool1 = max_pool_2x2(self.h_conv1)

        self.W_conv2 = weight_variable([5, 5, 32, 64])
        self.b_conv2 = bias_variable([64])

        self.h_conv2 = tf.nn.relu(conv2d(self.h_pool1, self.W_conv2) + self.b_conv2)
        self.h_pool2 = max_pool_2x2(self.h_conv2)

        self.W_fc1 = weight_variable([7*7*64, 2048])
        self.b_fc1 = bias_variable([2048])

        self.h_pool2_flat = tf.reshape(self.h_pool2, [-1, 7*7*64])
        self.h_fc1 = tf.nn.relu(tf.matmul(self.h_pool2_flat, self.W_fc1) + self.b_fc1)

        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')
        self.h_fc1_drop = tf.nn.dropout(self.h_fc1, self.keep_prob)

        self.W_fc2 = weight_variable([2048, 10])
        self.b_fc2 = bias_variable([10])

        self.tvars = [self.W_conv1, self.b_conv1, self.W_conv2, self.b_conv2, self.W_fc1, 
            self.b_fc1, self.W_fc2, self.b_fc2]

        self.weight_names = ['W_conv1', 'b_conv1', 'W_conv2', 'b_conv2', 'W_fc1', 
                'b_fc1', 'W_fc2', 'b_fc2']

        self.y_conv = tf.matmul(self.h_fc1_drop, self.W_fc2) + self.b_fc2

        self.error2 = tf.reduce_mean(
            tf.nn.softmax_cross_entropy_with_logits(labels= self.y_, logits= self.y_conv))
        self.train_step2 = tf.train.AdamOptimizer(1e-4).minimize(self.error2)


        '''Test Network Weights'''
        
        self.Wt_conv1 = weight_variable([5, 5, 1, 32])
        self.bt_conv1 = bias_variable([32])

        self.t_conv1 = tf.nn.sigmoid(conv2d(self.x_image, self.Wt_conv1) + self.bt_conv1)
        self.t_pool1 = max_pool_2x2(self.t_conv1)

        self.Wt_conv2 = weight_variable([5, 5, 32, 64])
        self.bt_conv2 = bias_variable([64])

        self.t_conv2 = tf.nn.relu(conv2d(self.t_pool1, self.Wt_conv2) + self.bt_conv2)
        self.t_pool2 = max_pool_2x2(self.t_conv2)

        self.t_pool2_flat = tf.reshape(self.t_pool2, [-1, 7*7*64])

        self.Wt_fc1 = weight_variable([7*7*64, 2048])
        self.bt_fc1 = bias_variable([2048])

        self.t_fc1 = tf.nn.relu(tf.matmul(self.t_pool2_flat, self.Wt_fc1) + self.bt_fc1)
        
        self.t_fc1_drop = tf.nn.dropout(self.t_fc1, self.keep_prob)

        self.Wt_fc2 = weight_variable([2048, 10])
        self.bt_fc2 = bias_variable([10])

        self.yt_conv = tf.matmul(self.t_fc1_drop, self.Wt_fc2) + self.bt_fc2

        self.error1 = tf.reduce_mean(
            tf.nn.softmax_cross_entropy_with_logits(labels= self.y_, logits= self.yt_conv))
        self.train_step1 = tf.train.AdamOptimizer(1e-4).minimize(self.error1)
        
    def set_weights(self,flat_len):
        
        self.ylearn = tf.placeholder(tf.float32, shape=[None, flat_len])
        self.Wlearn = weight_variable([2048,1])        
        self.blearn = bias_variable([1])        
        self.convlearn = tf.matmul(self.t_fc1_drop, self.Wlearn) + self.blearn
        self.W2learn = weight_variable([1,flat_len])
        self.b2learn = bias_variable([flat_len])
        self.convlearn2 = tf.matmul(self.convlearn, self.W2learn) + self.b2learn
        self.errorlearn = tf.reduce_mean(
            tf.losses.mean_squared_error(labels= self.ylearn, predictions= self.convlearn2))
        self.train_stepslearn = tf.train.AdamOptimizer(1e-4).minimize(self.errorlearn)
        return True
    


# Converts np.array to TEXT when inserting
sqlite3.register_adapter(np.ndarray, adapt_array)

# Converts TEXT to np.array when selecting
sqlite3.register_converter("array", convert_array)       

if __name__ == '__main__':
    train()

